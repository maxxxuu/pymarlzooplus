# --- Qatten specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 200000
evaluation_epsilon: 0.0

runner: "episode"

# Buffer
buffer_size: 5000
prioritized_buffer: False
prioritized_buffer_alpha: 0.6

# update the target network every {} episodes
target_update_interval: 200

obs_agent_id: True
obs_last_action: True
obs_individual_obs: False

mac: "cent_mac"
agent: "rnn_cent_pet"
standardise_rewards: True

# use the Q_Learner to train
agent_output_type: "q"
learner: "dmaq_qatten_clip_learner"
double_q: True
mixer: "dmaq"
use_rnn: True
#hidden_dim: 128
mixing_embed_dim: 32
hypernet_embed: 32
adv_hypernet_layers: 3
adv_hypernet_embed: 32

num_kernel: 10
is_minus_one: True
weighted_head: True

gamma: 0.99
batch_size: 32 # Number of episodes to train on

lr: 0.0005 # Learning rate for agents
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm
add_value_last_step: True
num_circle: 1 # Number of training iterations after each episode

name: "qplex_cent_pet"
