# --- MAT specific parameters ---

action_selector: "soft_policies"
mask_before_softmax: True                           

runner: "episode"

buffer_size: 10 # 10                                 
batch_size_run: 1 # 10                              
batch_size: 10 # 10                                  

# update the target network every {} training steps
target_update_interval_or_tau: 200                  

lr: 0.0003                                          

obs_agent_id: False                                  
obs_last_action: False                               
obs_individual_obs: False                           

mac: "non_shared_mac"
agent: "rnn_ns"
agent_output_type: "pi_logits"                      
learner: "mat_learner"
entropy_coef: 0.01                                  
use_rnn: False                                       
standardise_returns: False                          
standardise_rewards: True                           
q_nstep: 5                              # 1 corresponds to normal r + gammaV     
#critic_type: "cv_critic"                            
epochs: 4                                           
eps_clip: 0.2                           # !!same as clip parameter!!                                      
name: "mat_dec"                             # !!same as algorithmeps_clip parameter

t_max: 20050000 # 20050000                           

##    SEE MORE INFO AT THE END OF FILE
#     prepare parameters
algorithm_name: name                   # choices=["mat", "mat_dec", "mat_encoder", "mat_decoder", "mat_gru"]
experiment_name: "check"                # help="an identifier to distinguish different experiment."
seed: 1                                 # help="Random seed for numpy/torch")
cuda: True                              # !!same as use_cuda in default!! help="by default True, will use GPU to train; or else will use CPU;"
cuda_deterministic: True                # by default True, make sure random seed effective. if set, bypass such function.
n_training_threads: 1                   # help="Number of torch threads for training". Default 1
n_rollout_threads: 32                   # help="Number of parallel envs for training rollouts". Default 32
n_eval_rollout_threads: 1               # help="Number of parallel envs for evaluating rollouts". Default 1
n_render_rollout_threads: 1             # help="Number of parallel envs for rendering rollouts". Default 1
num_env_steps: 10e6                     # help='Number of environment steps to train (default: 10e6)'
user_name: "xxx"                        # help="[for wandb usage], to specify user's name for simply collecting training data.
use_wandb: False                        # help="[for wandb usage], by default True, will log date to wandb server. or else will use tensorboard to log data.

#     env parameters
#env_name: "StarCraft2"                  # help="specify the name of environment"
use_obs_instead_of_state: False         # help="Whether to use global state or concatenated obs"

#     replay buffer parameters
episode_length: 200                     # help="Max length for any episode"

#     network parameters
share_policy: True                      # help='Whether agent share the same policy'
use_centralized_V: True                 # help="Whether to use centralized V function"
stacked_frames: 1                       # help="Dimension of hidden layers for actor/critic networks"
use_stacked_frames: False               # help="Whether to use stacked_frames"
hidden_size: 64                         # help="Dimension of hidden layers for actor/critic networks" 
layer_N: 2                              # help="Number of layers for actor/critic networks"
use_ReLU: True                          # help="Whether to use ReLU"
use_popart: False                       # help="by default False, use PopArt to normalize rewards."
use_valuenorm: True                     # help="by default True, use running mean and std to normalize rewards."
use_feature_normalization: True         # help="Whether to apply layernorm to the inputs"
use_orthogonal: True                    # help="Whether to use Orthogonal initialization for weights and 0 initialization for biases"
gain: 0.01                              # help="The gain # of last action layer"

#     recurrent parameters
use_naive_recurrent_policy: False       # help='Whether to use a naive recurrent policy'
use_recurrent_policy: False             # help='use a recurrent policy'
recurrent_N: 1                          # help="The number of recurrent layers."
data_chunk_length: 10                   # help="Time length of chunks used to train a recurrent_policy"

#     optimizer parameters
#lr: 5e-4                                # help='learning rate (default: 5e-4)'
critic_lr: 5e-4                         # help='critic learning rate (default: 5e-4)'
opti_eps: 1e-5                          # help='RMSprop optimizer epsilon (default: 1e-5)'
weight_decay: 0  

#     ppo parameters
ppo_epoch: 15                           # help='number of ppo epochs (default: 15)'
use_clipped_value_loss: True            # help="by default, clip loss value. If set, do not clip loss value."
clip_param: 0.2                         # !!same as eps_clip parameter!! help='ppo clip parameter (default: 0.2)'   # 
num_mini_batch: 1                       # help='number of batches for ppo (default: 1)'
#entropy_coef: 0.01                      # help='entropy term coefficient (default: 0.01)'
value_loss_coef: 1                      # help='value loss coefficient (default: 0.5)'
use_max_grad_norm: True                 # help="by default, use max norm of gradients. If set, do not use."
max_grad_norm: 10.0                     # help='max norm of gradients (default: 0.5)'
use_gae: True                           # help='use generalized advantage estimation'
#gamma: 0.99                             # help='discount factor for rewards (default: 0.99)'
gae_lambda: 0.95                        # help='gae lambda parameter (default: 0.95)'
use_proper_time_limits: False           # help='compute returns taking into account time limits'
use_huber_loss: True                    # help="by default, use huber loss. If set, do not use huber loss."
use_value_active_masks: True            # help="by default True, whether to mask useless data in value loss."
use_policy_active_masks: True           # help="by default True, whether to mask useless data in policy loss."
huber_delta: 10.0                       # help=" coefficience of huber loss."

#     run parameters
use_linear_lr_decay: False              # help='use a linear schedule on the learning rate'
#     save parameters
save_interval: 100                      # help="time duration between contiunous twice models saving."

#     log parameters
log_interval: 5                         # help="time duration between contiunous twice log printing."

#     eval parameters
use_eval: False                         # help="by default, do not start evaluation. If set`, start evaluation alongside with training."
eval_interval: 25                       # help="time duration between contiunous twice evaluation progress."
eval_episodes: 32                       # help="number of episodes of a single evaluation."

#     render parameters
save_gifs: False                        # help="by default, do not save render video. If set, save video."
use_render: False                       # help="by default, do not render the env during training. If set, start render. Note: something, the environment has internal render process which is not controlled by this hyperparam."
render_episodes: 5                      # help="the number of episodes to render a given env"
ifi: 0.1                                # help="the play interval of each rendered image in saved video."

#     pretrained parameters
model_dir: None                         # help="by default None. set the path to pretrained model."

#     add for transformer
encode_state: False
n_block: 1
n_embd: 64
n_head: 1
dec_actor: False
share_actor: False

#      add for online multi-task
train_maps: None
eval_maps: None

#     Prepare parameters:
#         --algorithm_name <algorithm_name>
#             specifiy the algorithm, including `["mat", "mat_dec"]`
#         --experiment_name <str>
#             an identifier to distinguish different experiment.
#         --seed <int>
#             set seed for numpy and torch 
#         --cuda
#             by default True, will use GPU to train; or else will use CPU; 
#         --cuda_deterministic
#             by default, make sure random seed effective. if set, bypass such function.
#         --n_training_threads <int>
#             number of training threads working in parallel. by default 1
#         --n_rollout_threads <int>
#             number of parallel envs for training rollout. by default 32
#         --n_eval_rollout_threads <int>
#             number of parallel envs for evaluating rollout. by default 1
#         --n_render_rollout_threads <int>
#             number of parallel envs for rendering, could only be set as 1 for some environments.
#         --num_env_steps <int>
#             number of env steps to train (default: 10e6)
#         --user_name <str>
#             [for wandb usage], to specify user's name for simply collecting training data.
#         --use_wandb
#             [for wandb usage], by default True, will log date to wandb server. or else will use tensorboard to log data.
    
#     Env parameters:
#         --env_name <str>
#             specify the name of environment
#         --use_obs_instead_of_state
#             [only for some env] by default False, will use global state; or else will use concatenated local obs.
    
#     Replay Buffer parameters:
#         --episode_length <int>
#             the max length of episode in the buffer. 
    
#     Network parameters:
#         --share_policy
#             by default True, all agents will share the same network; set to make training agents use different policies. 
#         --use_centralized_V
#             by default True, use centralized training mode; or else will decentralized training mode.
#         --stacked_frames <int>
#             Number of input frames which should be stack together.
#         --hidden_size <int>
#             Dimension of hidden layers for actor/critic networks
#         --layer_N <int>
#             Number of layers for actor/critic networks
#         --use_ReLU
#             by default True, will use ReLU. or else will use Tanh.
#         --use_popart
#             by default True, use PopArt to normalize rewards. 
#         --use_valuenorm
#             by default True, use running mean and std to normalize rewards. 
#         --use_feature_normalization
#             by default True, apply layernorm to normalize inputs. 
#         --use_orthogonal
#             by default True, use Orthogonal initialization for weights and 0 initialization for biases. or else, will use xavier uniform inilialization.
#         --gain
#             by default 0.01, use the gain # of last action layer
#         --use_naive_recurrent_policy
#             by default False, use the whole trajectory to calculate hidden states.
#         --use_recurrent_policy
#             by default, use Recurrent Policy. If set, do not use.
#         --recurrent_N <int>
#             The number of recurrent layers ( default 1).
#         --data_chunk_length <int>
#             Time length of chunks used to train a recurrent_policy, default 10.
    
#     Optimizer parameters:
#         --lr <float>
#             learning rate parameter,  (default: 5e-4, fixed).
#         --critic_lr <float>
#             learning rate of critic  (default: 5e-4, fixed)
#         --opti_eps <float>
#             RMSprop optimizer epsilon (default: 1e-5)
#         --weight_decay <float>
#             coefficience of weight decay (default: 0)
    
#     PPO parameters:
#         --ppo_epoch <int>
#             number of ppo epochs (default: 15)
#         --use_clipped_value_loss 
#             by default, clip loss value. If set, do not clip loss value.
#         --clip_param <float>
#             ppo clip parameter (default: 0.2)
#         --num_mini_batch <int>
#             number of batches for ppo (default: 1)
#         --entropy_coef <float>
#             entropy term coefficient (default: 0.01)
#         --use_max_grad_norm 
#             by default, use max norm of gradients. If set, do not use.
#         --max_grad_norm <float>
#             max norm of gradients (default: 0.5)
#         --use_gae
#             by default, use generalized advantage estimation. If set, do not use gae.
#         --gamma <float>
#             discount factor for rewards (default: 0.99)
#         --gae_lambda <float>
#             gae lambda parameter (default: 0.95)
#         --use_proper_time_limits
#             by default, the return value does consider limits of time. If set, compute returns with considering time limits factor.
#         --use_huber_loss
#             by default, use huber loss. If set, do not use huber loss.
#         --use_value_active_masks
#             by default True, whether to mask useless data in value loss.  
#         --huber_delta <float>
#             coefficient of huber loss.  
    
#     PPG parameters:
#         --aux_epoch <int>
#             number of auxiliary epochs. (default: 4)
#         --clone_coef <float>
#             clone term coefficient (default: 0.01)
    
#     Run parametersï¼š
#         --use_linear_lr_decay
#             by default, do not apply linear decay to learning rate. If set, use a linear schedule on the learning rate
    
#     Save & Log parameters:
#         --save_interval <int>
#             time duration between contiunous twice models saving.
#         --log_interval <int>
#             time duration between contiunous twice log printing.
    
#     Eval parameters:
#         --use_eval
#             by default, do not start evaluation. If set`, start evaluation alongside with training.
#         --eval_interval <int>
#             time duration between contiunous twice evaluation progress.
#         --eval_episodes <int>
#             number of episodes of a single evaluation.
    
#     Render parameters:
#         --save_gifs
#             by default, do not save render video. If set, save video.
#         --use_render
#             by default, do not render the env during training. If set, start render. Note: something, the environment has internal render process which is not controlled by this hyperparam.
#         --render_episodes <int>
#             the number of episodes to render a given env
#         --ifi <float>
#             the play interval of each rendered image in saved video.
    
#     Pretrained parameters:
#         --model_dir <str>
#             by default None. set the path to pretrained model.
